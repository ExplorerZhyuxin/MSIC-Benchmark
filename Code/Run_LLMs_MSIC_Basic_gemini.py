# =================================================================
# 1. å¯¼å…¥æ‰€æœ‰éœ€è¦çš„â€œå·¥å…·åŒ…â€
# =================================================================
import os
import glob
import time
import logging
import json # ã€å·²æ·»åŠ ã€‘å¯¼å…¥jsonåº“
import re   # ã€å·²æ·»åŠ ã€‘å¯¼å…¥reåº“
import requests # ã€å·²æ·»åŠ ã€‘å¯¼å…¥requestsåº“
import pandas as pd
from tqdm import tqdm
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain_openai import OpenAIEmbeddings # ä¸¤ä¸ªéƒ½å¯¼å…¥ä»¥é˜²ä¸‡ä¸€
from langchain.vectorstores import Chroma

# =================================================================
# 2. å…¨å±€é…ç½® (Global Configuration)
# =================================================================
# --- LLM å’Œ Embedding æ¨¡å‹é…ç½® ---
LLM_MODEL = "gemini-2.5-pro"
EMBEDDING_MODEL = "text-embedding-3-small" # Embeddingæ¨¡å‹åç§°
MAX_RETRIES = 5
TEMPERATURE = 0.3

# --- API å…³é”®ä¿¡æ¯ ---
# ã€å·²ä¿®æ”¹ã€‘å°†APIä¿¡æ¯æ”¾åœ¨è¿™é‡Œï¼Œæ›´æ¸…æ™°
API_URL = ""
API_KEY = ""

# --- æ–‡ä»¶å’Œè·¯å¾„é…ç½® ---
PERSIST_DIRECTORY = r"chroma_db"
BENCHMARK_FILE = r"MSIC_basic_bench.xlsx"
OUTPUT_FOLDER = r""
OUTPUT_FILENAME = os.path.join(OUTPUT_FOLDER, f"MSIC_basic_bench_results-{LLM_MODEL}.csv")


# ã€å·²ä¿®æ”¹ã€‘è®¾ç½®Langchainéœ€è¦çš„ç¯å¢ƒå˜é‡ï¼Œå³ä½¿æˆ‘ä»¬ä¸ç›´æ¥ç”¨å®ƒçš„LLMè°ƒç”¨ï¼ŒEmbeddingå¯èƒ½éœ€è¦

os.environ["OPENAI_API_KEY"] = API_KEY
EMBEDDING_API_BASE = ""
# --- Prompt æ¨¡æ¿å®šä¹‰ ---
PROMPT_VANILLA = """
You are a biomedical expert. Answer the following question based on your internal knowledge.

Question: {question}
"""

PROMPT_COT = """
You are a biomedical expert. Answer the following question. First, provide your step-by-step reasoning process. Then, provide the final answer.

Let's think step by step.

Question: {question}

Reasoning:
[Your step-by-step reasoning here]

Final Answer:
[Your answer here]
"""

PROMPT_ROT = """
You are a biomedical expert. Answer the following question. 

Question: {question}

Imagine 3 medical experts are solving this task. Each expert independently provides their step-by-step reasoning and final answer.
After all experts have finished, they discuss together, review and backtrack their previous reasoning steps, and finally reach a consensus on the final answer.
Please present:
[Expert 1's reasoning and answer],
[Expert 2's reasoning and answer],
[Expert 3's reasoning and answer],
[The discussion and the agreed final answer]
"""

RAG_TEMPLATE = """
You are a biomedical expert. Answer the following question based ONLY on the provided clinical guideline context.

Question: {question}

Context:
---
{context}
---
"""

# =================================================================
# 3. å‘é‡æ•°æ®åº“ (Vector Database) åˆ›å»ºä¸åŠ è½½
# =================================================================
def create_and_save_vector_db():
    print("--- æ­¥éª¤ 1/3: æœªå‘ç°æœ¬åœ°å‘é‡æ•°æ®åº“ï¼Œå¼€å§‹åˆ›å»º... ---")
    pdf_paths = glob.glob('Knowledge_Sources/*.pdf')
    print(f"  > å‘ç° {len(pdf_paths)} ä¸ªPDFæ–‡ä»¶: {[os.path.basename(p) for p in pdf_paths]}")
    
    all_documents = []
    for pdf_path in pdf_paths:
        loader = PyPDFLoader(pdf_path)
        documents = loader.load()
        all_documents.extend(documents)
    print(f'  > PDFåŠ è½½å®Œæˆï¼Œå…±åˆ‡åˆ†æˆ {len(all_documents)} ä¸ªæ–‡æ¡£é¡µé¢ã€‚')

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    texts = text_splitter.split_documents(all_documents)
    print(f'  > æ–‡æ¡£å—åˆ‡åˆ†å®Œæˆï¼Œå…±å¾—åˆ° {len(texts)} ä¸ªæ–‡æœ¬å—ã€‚')

    print("  > å¼€å§‹åˆ›å»ºå¹¶ä¿å­˜å‘é‡æ•°æ®åº“ï¼Œè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´...")
    vectordb = Chroma.from_documents(
        documents=texts,
        embedding=OpenAIEmbeddings(model=EMBEDDING_MODEL, openai_api_base=EMBEDDING_API_BASE), # æŒ‡å®šæ¨¡å‹
        persist_directory=PERSIST_DIRECTORY
    )
    vectordb.persist()
    print("--- å‘é‡æ•°æ®åº“åˆ›å»ºå¹¶ä¿å­˜æˆåŠŸï¼ ---")
    return vectordb

def load_vector_db():
    print("--- æ­¥éª¤ 1/3: å‘ç°å·²ä¿å­˜çš„å‘é‡æ•°æ®åº“ï¼Œç›´æ¥åŠ è½½... ---")
    vectordb = Chroma(
        persist_directory=PERSIST_DIRECTORY,
        embedding_function=OpenAIEmbeddings(model=EMBEDDING_MODEL, openai_api_base=EMBEDDING_API_BASE) # æŒ‡å®šæ¨¡å‹
    )
    metadatas = vectordb.get().get('metadatas', [])
    if metadatas:
        sources = set(m.get('source') for m in metadatas if 'source' in m)
        print("  > ç”¨äºæ£€ç´¢çš„æ–‡ä»¶:", [os.path.basename(s) for s in sources])
    else:
        print("  > å‘é‡æ•°æ®åº“ä¸ºç©ºæˆ–æ— æ³•è·å–å…ƒæ•°æ®ã€‚")
    print("--- å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸï¼ ---")
    return vectordb

if not os.path.exists(PERSIST_DIRECTORY):
    vectordb = create_and_save_vector_db()
else:
    vectordb = load_vector_db()

# =================================================================
# 4. API è°ƒç”¨å‡½æ•° (ç»Ÿä¸€ä½¿ç”¨ requests)
# =================================================================

# ã€å·²ä¿®æ”¹ã€‘è¿™æ˜¯ä½ éªŒè¯è¿‡æˆåŠŸçš„APIè°ƒç”¨å‡½æ•°ï¼Œæˆ‘ä»¬ç”¨å®ƒæ¥å¤„ç†æ‰€æœ‰éRAGçš„è¯·æ±‚
def fetch_prompts_response(question, PROMPT, max_retries=MAX_RETRIES, sleep_duration=2):
    content = PROMPT.format(question=question)
    headers = {
       'Authorization': f'Bearer {API_KEY}',
       'Content-Type': 'application/json'
    }
    payload = json.dumps({
        "model": LLM_MODEL,
        "messages": [{"role": "user", "content": content}],
        "max_tokens": 4096,
        "temperature": TEMPERATURE,
        "stream": False
    })

    for attempt in range(max_retries):
        try:
            response = requests.post(API_URL, headers=headers, data=payload, timeout=180)
            response.raise_for_status()
            response_data = response.json()
            if 'choices' in response_data and len(response_data['choices']) > 0:
                return response_data['choices'][0]['message']['content'].strip()
            else:
                raise ValueError("APIè¿”å›æ ¼å¼ä¸æ­£ç¡®")
        except Exception as e:
            logging.error(f"API è°ƒç”¨å¤±è´¥ (Attempt {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(sleep_duration)
            else:
                return f"ERROR: API call failed after {max_retries} retries." # è¿”å›é”™è¯¯ä¿¡æ¯è€Œä¸æ˜¯å´©æºƒ
    return None

# ã€å·²ä¿®æ”¹ã€‘è¿™æ˜¯æ–°çš„RAGå“åº”å‡½æ•°ï¼Œå®ƒæ‰‹åŠ¨æ‰§è¡Œæ£€ç´¢ã€æ„å»ºPromptã€ç„¶åè°ƒç”¨ä¸Šé¢çš„å‡½æ•°
def fetch_RAG_response(question, max_retries=MAX_RETRIES):
    try:
        # æ­¥éª¤ 1: ä½¿ç”¨Langchainçš„retrieverè¿›è¡Œæ–‡æ¡£æ£€ç´¢
        retriever = vectordb.as_retriever(search_type="similarity", search_kwargs={"k": 3})
        retrieved_docs = retriever.get_relevant_documents(question)
        
        # æ­¥éª¤ 2: å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹æ ¼å¼åŒ–ä¸ºä¸Šä¸‹æ–‡
        context = "\n\n---\n\n".join([doc.page_content for doc in retrieved_docs])
        
        # æ­¥éª¤ 3: ä½¿ç”¨æˆ‘ä»¬è‡ªå·±çš„APIå‡½æ•°æ¥è·å–å›ç­”
        # æ³¨æ„è¿™é‡Œæˆ‘ä»¬ä¼ å…¥çš„æ˜¯RAG_TEMPLATEï¼Œè€Œä¸æ˜¯å…¶ä»–æ¨¡æ¿
        answer = fetch_prompts_response(
            question=question,
            PROMPT=RAG_TEMPLATE.format(question="{question}", context=context), # æ‰‹åŠ¨å¡«å……context
            max_retries=max_retries
        )
        return answer
    except Exception as e:
        logging.error(f"RAGæµç¨‹å¤±è´¥: {e}")
        return f"ERROR: RAG process failed: {e}"

# =================================================================
# 5. ä¸»æ‰§è¡Œé€»è¾‘ (Main Execution Logic)
# =================================================================
# å®šä¹‰é”™è¯¯ä¿¡æ¯ï¼Œæ–¹ä¾¿åé¢åˆ¤æ–­
ERROR_MSG = "ERROR: API call failed after 5 retries."

# åŠ è½½å®Œæ•´çš„ benchmark è¡¨æ ¼
basic_benchmark = pd.read_excel(BENCHMARK_FILE, sheet_name="Sheet1")

# åˆå§‹åŒ–ç»“æœåˆ—è¡¨å’Œå·²å®Œæˆé—®é¢˜é›†åˆ
results = []
completed_questions = set()

# ã€æ–­ç‚¹ç»­è·‘é€»è¾‘ã€‘
if os.path.exists(OUTPUT_FILENAME):
    print(f"--- å‘ç°å·²å­˜åœ¨çš„ç»“æœæ–‡ä»¶ '{OUTPUT_FILENAME}'ï¼Œå¯åŠ¨æ–­ç‚¹ç»­è·‘æ¨¡å¼... ---")
    df_existing = pd.read_csv(OUTPUT_FILENAME)
    
    # æ‰¾å‡ºæ‰€æœ‰4ä¸ªå“åº”åˆ—éƒ½ä¸åŒ…å«é”™è¯¯ä¿¡æ¯çš„è¡Œï¼Œè¿™äº›æ˜¯å®Œå…¨æˆåŠŸçš„
    successful_rows = df_existing[
        (~df_existing['Vanilla_response'].astype(str).str.contains("ERROR", na=False)) &
        (~df_existing['COT_response'].astype(str).str.contains("ERROR", na=False)) &
        (~df_existing['ROT_response'].astype(str).str.contains("ERROR", na=False)) &
        (~df_existing['RAG_response'].astype(str).str.contains("ERROR", na=False))
    ]
    
    # å°†å·²å®Œæˆçš„é—®é¢˜åŠ å…¥é›†åˆï¼Œæ–¹ä¾¿å¿«é€ŸæŸ¥æ‰¾
    completed_questions = set(successful_rows['question'])
    print(f"  > å·²æˆåŠŸå¤„ç† {len(completed_questions)} / {len(basic_benchmark)} ä¸ªé—®é¢˜ï¼Œå°†è·³è¿‡å®ƒä»¬ã€‚")
    
    # å°†å·²æˆåŠŸçš„ç»“æœå…ˆåŠ è½½åˆ°ç»“æœåˆ—è¡¨ä¸­
    results = successful_rows.to_dict('records')
else:
    print(f"--- æœªå‘ç°ç»“æœæ–‡ä»¶ï¼Œå°†ä»å¤´å¼€å§‹è¿è¡Œ... ---")

# å¼€å§‹å¤„ç†...
print(f"\n--- å¼€å§‹ä½¿ç”¨æ¨¡å‹ '{LLM_MODEL}' å¤„ç† Benchmark... ---")
for index, row in tqdm(basic_benchmark.iterrows(), total=basic_benchmark.shape[0], desc="å¤„ç†Benchmarké—®é¢˜"):
    question = str(row['question'])

    # ã€æ–­ç‚¹ç»­è·‘æ ¸å¿ƒæ£€æŸ¥ã€‘
    if question in completed_questions:
        continue # å¦‚æœè¿™ä¸ªé—®é¢˜å·²ç»å¤„ç†è¿‡ï¼Œç›´æ¥è·³åˆ°ä¸‹ä¸€ä¸ª

    # --- å¦‚æœæ˜¯æœªå®Œæˆçš„é—®é¢˜ï¼Œåˆ™æ­£å¸¸æ‰§è¡Œæ‰€æœ‰APIè°ƒç”¨ ---
    question_type = row['type']
    full_question = f"{question_type}: {question}"
    domain_topic = row['Domain_Topic']
    ground_truth = row['answer']

    print(f"\nç»§ç»­å¤„ç†é—®é¢˜ {index + 1}: {full_question[:100]}...")

    vanilla_response = fetch_prompts_response(full_question, PROMPT_VANILLA)
    cot_response = fetch_prompts_response(full_question, PROMPT_COT)
    rot_response = fetch_prompts_response(full_question, PROMPT_ROT)
    rag_response = fetch_RAG_response(full_question)

    results.append({
        "type": question_type,
        "question": question,
        "domain_topic": domain_topic,
        "ground_truth": ground_truth,
        "Vanilla_response": vanilla_response,
        "COT_response": cot_response,
        "ROT_response": rot_response,
        "RAG_response": rag_response,
    })

# =================================================================
# 6. ä¿å­˜ç»“æœ
# =================================================================
print(f"\n--- æ‰€æœ‰é—®é¢˜å¤„ç†å®Œæ¯•ï¼Œæ­£åœ¨åˆå¹¶å¹¶ä¿å­˜æœ€ç»ˆç»“æœ... ---")
final_df = pd.DataFrame(results)

# ã€é‡è¦ã€‘ä¸ºäº†ä¿è¯é¡ºåºå’ŒåŸå§‹benchmarkä¸€è‡´ï¼Œæˆ‘ä»¬åšä¸€ä¸ªæ’åº
# åˆ›å»ºä¸€ä¸ªé—®é¢˜åˆ°é¡ºåºçš„æ˜ å°„
question_order = {str(q): i for i, q in enumerate(basic_benchmark['question'])}
# æ ¹æ®è¿™ä¸ªé¡ºåºç»™ final_df æ’åº
final_df['sort_order'] = final_df['question'].map(question_order)
final_df = final_df.sort_values('sort_order').drop(columns=['sort_order'])

final_df.to_csv(OUTPUT_FILENAME, index=False, encoding="utf-8-sig")
print(f"ğŸ‰ ä»»åŠ¡å®Œæˆï¼ç»“æœå·²æ›´æ–°å¹¶ä¿å­˜è‡³: {OUTPUT_FILENAME}")