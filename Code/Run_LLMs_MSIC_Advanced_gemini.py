# =================================================================
# 1. å¯¼å…¥æ‰€æœ‰éœ€è¦çš„â€œå·¥å…·åŒ…â€
# =================================================================
import os
import glob
import time
import logging
import json # ã€å·²æ·»åŠ ã€‘å¯¼å…¥jsonåº“
import re   # ã€å·²æ·»åŠ ã€‘å¯¼å…¥reåº“
import requests # ã€å·²æ·»åŠ ã€‘å¯¼å…¥requestsåº“
import pandas as pd
from tqdm import tqdm
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain_openai import OpenAIEmbeddings # ä¸¤ä¸ªéƒ½å¯¼å…¥ä»¥é˜²ä¸‡ä¸€
from langchain.vectorstores import Chroma

# =================================================================
# 2. å…¨å±€é…ç½® (Global Configuration)
# =================================================================
# --- LLM å’Œ Embedding æ¨¡å‹é…ç½® ---
LLM_MODEL = "gemini-2.5-pro"
EMBEDDING_MODEL = "text-embedding-3-small" # Embeddingæ¨¡å‹åç§°
MAX_RETRIES = 5
TEMPERATURE = 0.3

# --- API å…³é”®ä¿¡æ¯ ---
# ã€å·²ä¿®æ”¹ã€‘å°†APIä¿¡æ¯æ”¾åœ¨è¿™é‡Œï¼Œæ›´æ¸…æ™°
API_URL = ""
API_KEY = ""

# --- æ–‡ä»¶å’Œè·¯å¾„é…ç½® ---
PERSIST_DIRECTORY = r"chroma_db"
BENCHMARK_FILE = r"MSIC_advance_bench_marked.xlsx"
OUTPUT_FOLDER = ""
OUTPUT_FILENAME = os.path.join(OUTPUT_FOLDER, f"MSIC_advanced_bench_results-{LLM_MODEL}.csv")


# ã€å·²ä¿®æ”¹ã€‘è®¾ç½®Langchainéœ€è¦çš„ç¯å¢ƒå˜é‡ï¼Œå³ä½¿æˆ‘ä»¬ä¸ç›´æ¥ç”¨å®ƒçš„LLMè°ƒç”¨ï¼ŒEmbeddingå¯èƒ½éœ€è¦

os.environ["OPENAI_API_KEY"] = API_KEY
EMBEDDING_API_BASE = ""
# --- Prompt æ¨¡æ¿å®šä¹‰ ---
PROMPT_VANILLA = """
You are a biomedical expert. Answer the following question based on your internal knowledge.

Question: {question}
"""

PROMPT_COT = """
You are a biomedical expert. Answer the following question. First, provide your step-by-step reasoning process. Then, provide the final answer.

Let's think step by step.

Question: {question}

Reasoning:
[Your step-by-step reasoning here]

Final Answer:
[Your answer here]
"""

PROMPT_ROT = """
You are a biomedical expert. Answer the following question. 

Question: {question}

Imagine 3 medical experts are solving this task. Each expert independently provides their step-by-step reasoning and final answer.
After all experts have finished, they discuss together, review and backtrack their previous reasoning steps, and finally reach a consensus on the final answer.
Please present:
[Expert 1's reasoning and answer],
[Expert 2's reasoning and answer],
[Expert 3's reasoning and answer],
[The discussion and the agreed final answer]
"""

RAG_TEMPLATE = """
You are a biomedical expert. Answer the following question based ONLY on the provided clinical guideline context.

Question: {question}

Context:
---
{context}
---
"""

# =================================================================
# 3. å‘é‡æ•°æ®åº“ (Vector Database) åˆ›å»ºä¸åŠ è½½
# =================================================================
def create_and_save_vector_db():
    print("--- æ­¥éª¤ 1/3: æœªå‘ç°æœ¬åœ°å‘é‡æ•°æ®åº“ï¼Œå¼€å§‹åˆ›å»º... ---")
    pdf_paths = glob.glob('Knowledge_Sources/*.pdf')
    print(f"  > å‘ç° {len(pdf_paths)} ä¸ªPDFæ–‡ä»¶: {[os.path.basename(p) for p in pdf_paths]}")
    
    all_documents = []
    for pdf_path in pdf_paths:
        loader = PyPDFLoader(pdf_path)
        documents = loader.load()
        all_documents.extend(documents)
    print(f'  > PDFåŠ è½½å®Œæˆï¼Œå…±åˆ‡åˆ†æˆ {len(all_documents)} ä¸ªæ–‡æ¡£é¡µé¢ã€‚')

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    texts = text_splitter.split_documents(all_documents)
    print(f'  > æ–‡æ¡£å—åˆ‡åˆ†å®Œæˆï¼Œå…±å¾—åˆ° {len(texts)} ä¸ªæ–‡æœ¬å—ã€‚')

    print("  > å¼€å§‹åˆ›å»ºå¹¶ä¿å­˜å‘é‡æ•°æ®åº“ï¼Œè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´...")
    vectordb = Chroma.from_documents(
        documents=texts,
        embedding=OpenAIEmbeddings(model=EMBEDDING_MODEL, openai_api_base=EMBEDDING_API_BASE), # æŒ‡å®šæ¨¡å‹
        persist_directory=PERSIST_DIRECTORY
    )
    vectordb.persist()
    print("--- å‘é‡æ•°æ®åº“åˆ›å»ºå¹¶ä¿å­˜æˆåŠŸï¼ ---")
    return vectordb

def load_vector_db():
    print("--- æ­¥éª¤ 1/3: å‘ç°å·²ä¿å­˜çš„å‘é‡æ•°æ®åº“ï¼Œç›´æ¥åŠ è½½... ---")
    vectordb = Chroma(
        persist_directory=PERSIST_DIRECTORY,
        embedding_function=OpenAIEmbeddings(model=EMBEDDING_MODEL, openai_api_base=EMBEDDING_API_BASE) # æŒ‡å®šæ¨¡å‹
    )
    metadatas = vectordb.get().get('metadatas', [])
    if metadatas:
        sources = set(m.get('source') for m in metadatas if 'source' in m)
        print("  > ç”¨äºæ£€ç´¢çš„æ–‡ä»¶:", [os.path.basename(s) for s in sources])
    else:
        print("  > å‘é‡æ•°æ®åº“ä¸ºç©ºæˆ–æ— æ³•è·å–å…ƒæ•°æ®ã€‚")
    print("--- å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸï¼ ---")
    return vectordb

if not os.path.exists(PERSIST_DIRECTORY):
    vectordb = create_and_save_vector_db()
else:
    vectordb = load_vector_db()

# =================================================================
# 4. API è°ƒç”¨å‡½æ•° (ç»Ÿä¸€ä½¿ç”¨ requests)
# =================================================================

# ã€å·²ä¿®æ”¹ã€‘è¿™æ˜¯ä½ éªŒè¯è¿‡æˆåŠŸçš„APIè°ƒç”¨å‡½æ•°ï¼Œæˆ‘ä»¬ç”¨å®ƒæ¥å¤„ç†æ‰€æœ‰éRAGçš„è¯·æ±‚
def fetch_prompts_response(question, PROMPT, max_retries=MAX_RETRIES, sleep_duration=2):
    content = PROMPT.format(question=question)
    headers = {
       'Authorization': f'Bearer {API_KEY}',
       'Content-Type': 'application/json'
    }
    payload = json.dumps({
        "model": LLM_MODEL,
        "messages": [{"role": "user", "content": content}],
        "max_tokens": 4096,
        "temperature": TEMPERATURE,
        "stream": False
    })

    for attempt in range(max_retries):
        try:
            response = requests.post(API_URL, headers=headers, data=payload, timeout=180)
            response.raise_for_status()
            response_data = response.json()
            if 'choices' in response_data and len(response_data['choices']) > 0:
                return response_data['choices'][0]['message']['content'].strip()
            else:
                raise ValueError("APIè¿”å›æ ¼å¼ä¸æ­£ç¡®")
        except Exception as e:
            logging.error(f"API è°ƒç”¨å¤±è´¥ (Attempt {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(sleep_duration)
            else:
                return f"ERROR: API call failed after {max_retries} retries." # è¿”å›é”™è¯¯ä¿¡æ¯è€Œä¸æ˜¯å´©æºƒ
    return None

# ã€å·²ä¿®æ”¹ã€‘è¿™æ˜¯æ–°çš„RAGå“åº”å‡½æ•°ï¼Œå®ƒæ‰‹åŠ¨æ‰§è¡Œæ£€ç´¢ã€æ„å»ºPromptã€ç„¶åè°ƒç”¨ä¸Šé¢çš„å‡½æ•°
def fetch_RAG_response(question, max_retries=MAX_RETRIES):
    try:
        # æ­¥éª¤ 1: ä½¿ç”¨Langchainçš„retrieverè¿›è¡Œæ–‡æ¡£æ£€ç´¢
        retriever = vectordb.as_retriever(search_type="similarity", search_kwargs={"k": 3})
        retrieved_docs = retriever.get_relevant_documents(question)
        
        # æ­¥éª¤ 2: å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹æ ¼å¼åŒ–ä¸ºä¸Šä¸‹æ–‡
        context = "\n\n---\n\n".join([doc.page_content for doc in retrieved_docs])
        
        # æ­¥éª¤ 3: ä½¿ç”¨æˆ‘ä»¬è‡ªå·±çš„APIå‡½æ•°æ¥è·å–å›ç­”
        # æ³¨æ„è¿™é‡Œæˆ‘ä»¬ä¼ å…¥çš„æ˜¯RAG_TEMPLATEï¼Œè€Œä¸æ˜¯å…¶ä»–æ¨¡æ¿
        answer = fetch_prompts_response(
            question=question,
            PROMPT=RAG_TEMPLATE.format(question="{question}", context=context), # æ‰‹åŠ¨å¡«å……context
            max_retries=max_retries
        )
        return answer
    except Exception as e:
        logging.error(f"RAGæµç¨‹å¤±è´¥: {e}")
        return f"ERROR: RAG process failed: {e}"

# =================================================================
# 5. ä¸»æ‰§è¡Œé€»è¾‘ (Main Execution Logic)
# =================================================================
ERROR_MSG = "ERROR: API call failed after 5 retries."
try:
    benchmark_df = pd.read_excel(BENCHMARK_FILE)
except FileNotFoundError:
    print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°Benchmarkæ–‡ä»¶ '{BENCHMARK_FILE}'ã€‚è¯·æ£€æŸ¥è·¯å¾„ã€‚")
    exit()

results = []
completed_questions = set()

# ã€æ–­ç‚¹ç»­è·‘é€»è¾‘ã€‘
if os.path.exists(OUTPUT_FILENAME):
    print(f"--- å‘ç°å·²å­˜åœ¨çš„ç»“æœæ–‡ä»¶ '{OUTPUT_FILENAME}'ï¼Œå¯åŠ¨æ–­ç‚¹ç»­è·‘æ¨¡å¼... ---")
    df_existing = pd.read_csv(OUTPUT_FILENAME)
    # ç­›é€‰å®Œå…¨æˆåŠŸçš„è¡Œ
    successful_rows = df_existing.dropna(subset=['Vanilla_response', 'COT_response', 'ROT_response', 'RAG_response'])
    successful_rows = successful_rows[
        (~successful_rows['Vanilla_response'].astype(str).str.contains("ERROR", na=False)) &
        (~successful_rows['COT_response'].astype(str).str.contains("ERROR", na=False)) &
        (~successful_rows['ROT_response'].astype(str).str.contains("ERROR", na=False)) &
        (~successful_rows['RAG_response'].astype(str).str.contains("ERROR", na=False))
    ]
    # ä½¿ç”¨ 'marked_question' ä½œä¸ºå”¯ä¸€æ ‡è¯†ç¬¦
    completed_questions = set(successful_rows['marked_question'])
    print(f"  > å·²æˆåŠŸå¤„ç† {len(completed_questions)} / {len(benchmark_df)} ä¸ªé—®é¢˜ï¼Œå°†è·³è¿‡å®ƒä»¬ã€‚")
    results = successful_rows.to_dict('records')
else:
    print(f"--- æœªå‘ç°ç»“æœæ–‡ä»¶ï¼Œå°†ä»å¤´å¼€å§‹è¿è¡Œ... ---")

print(f"\n--- å¼€å§‹ä½¿ç”¨æ¨¡å‹ '{LLM_MODEL}' å¤„ç† Benchmark... ---")
for index, row in tqdm(benchmark_df.iterrows(), total=benchmark_df.shape[0], desc="å¤„ç†Benchmarké—®é¢˜"):
    # ã€è¦æ±‚1ã€‘ä½¿ç”¨ marked_question åˆ—ä½œä¸ºå”¯ä¸€æ ‡è¯†ç¬¦å’Œå†…å®¹æ¥æº
    marked_question = str(row['marked_question'])
    
    # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆ
    if marked_question in completed_questions:
        continue

    # ã€è¦æ±‚3ã€‘è‡ªåŠ¨æ¸…ç† (**) æ ‡è®°
    cleaned_question = marked_question.replace('(**)', '').strip()
    
    # ã€è¦æ±‚2ã€‘é€‚é… advanced æ–‡ä»¶çš„æ‰€æœ‰åˆ—å
    task = row['Task']
    subtask = row['Subtask']
    reference_answer = row['Reference Answer']
    
    # æ„å»ºæœ€ç»ˆæé—®æ–‡æœ¬
    full_question_to_ask = f"{task}: {cleaned_question}"

    print(f"\nå¤„ç†é—®é¢˜ {index + 1}: {full_question_to_ask[:100]}...")

    vanilla_response = fetch_prompts_response(full_question_to_ask, PROMPT_VANILLA)
    cot_response = fetch_prompts_response(full_question_to_ask, PROMPT_COT)
    rot_response = fetch_prompts_response(full_question_to_ask, PROMPT_ROT)
    rag_response = fetch_RAG_response(full_question_to_ask)

    # ä¿å­˜ç»“æœæ—¶ï¼Œä¿ç•™åŸå§‹ marked_question ä»¥ä¾¿è¿½æº¯å’Œä½œä¸ºå”¯ä¸€é”®
    results.append({
        'Task': task,
        'Subtask': subtask,
        'Question': row['Question'], # ä¿ç•™åŸå§‹å¹²å‡€é—®é¢˜
        'marked_question': marked_question, # ä¿ç•™åŸå§‹å¸¦æ ‡è®°é—®é¢˜
        'Reference Answer': reference_answer,
        'Reference Article': row['Reference Article'],
        'Importance_Score': row['Importance_Score'],
        'Clarity_Score': row['Clarity_Score'],
        'Vanilla_response': vanilla_response,
        'COT_response': cot_response,
        'ROT_response': rot_response,
        'RAG_response': rag_response,
    })
    # break
# =================================================================
# 6. ä¿å­˜ç»“æœ (Save Results) - ã€æœ€ç»ˆç‰ˆã€‘
# =================================================================
print(f"\n--- æ‰€æœ‰é—®é¢˜å¤„ç†å®Œæ¯•ï¼Œæ­£åœ¨åˆå¹¶å¹¶ä¿å­˜æœ€ç»ˆç»“æœ... ---")
if results:
    final_df = pd.DataFrame(results)
    
    # ä½¿ç”¨ 'marked_question' ä½œä¸ºæ’åºä¾æ®ï¼Œç¡®ä¿é¡ºåºä¸åŸå§‹æ–‡ä»¶ä¸€è‡´
    question_order = {str(q): i for i, q in enumerate(benchmark_df['marked_question'])}
    final_df['sort_order'] = final_df['marked_question'].map(question_order)
    final_df = final_df.sort_values('sort_order').drop(columns=['sort_order'])

    final_df.to_csv(OUTPUT_FILENAME, index=False, encoding="utf-8-sig")
    print(f"ğŸ‰ ä»»åŠ¡å®Œæˆï¼ç»“æœå·²æ›´æ–°å¹¶ä¿å­˜è‡³: {OUTPUT_FILENAME}")
else:
    print("æ²¡æœ‰æ–°çš„ç»“æœéœ€è¦ä¿å­˜ã€‚")