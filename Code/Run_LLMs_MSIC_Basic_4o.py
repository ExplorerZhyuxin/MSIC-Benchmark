# =================================================================
# 1. å¯¼å…¥æ‰€æœ‰éœ€è¦çš„â€œå·¥å…·åŒ…â€
# =================================================================
import os
import glob
import time
import logging
import json # ã€å·²æ·»åŠ ã€‘å¯¼å…¥jsonåº“
import re   # ã€å·²æ·»åŠ ã€‘å¯¼å…¥reåº“
import requests # ã€å·²æ·»åŠ ã€‘å¯¼å…¥requestsåº“
import pandas as pd
from tqdm import tqdm
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain_openai import OpenAIEmbeddings # ä¸¤ä¸ªéƒ½å¯¼å…¥ä»¥é˜²ä¸‡ä¸€
from langchain.vectorstores import Chroma

# =================================================================
# 2. å…¨å±€é…ç½® (Global Configuration)
# =================================================================
# --- LLM å’Œ Embedding æ¨¡å‹é…ç½® ---
LLM_MODEL = "gpt-4o-2024-08-06"
EMBEDDING_MODEL = "text-embedding-3-small" # Embeddingæ¨¡å‹åç§°
MAX_RETRIES = 5
TEMPERATURE = 0.3

# --- API å…³é”®ä¿¡æ¯ ---
# ã€å·²ä¿®æ”¹ã€‘å°†APIä¿¡æ¯æ”¾åœ¨è¿™é‡Œï¼Œæ›´æ¸…æ™°
API_URL = ""
API_KEY = ""

# --- æ–‡ä»¶å’Œè·¯å¾„é…ç½® ---
PERSIST_DIRECTORY = r"chroma_db"
BENCHMARK_FILE = r"MSIC_basic_bench.xlsx"
OUTPUT_FOLDER = r""
OUTPUT_FILENAME = os.path.join(OUTPUT_FOLDER, f"MSIC_basic_bench_results-{LLM_MODEL}.csv")


# ã€å·²ä¿®æ”¹ã€‘è®¾ç½®Langchainéœ€è¦çš„ç¯å¢ƒå˜é‡ï¼Œå³ä½¿æˆ‘ä»¬ä¸ç›´æ¥ç”¨å®ƒçš„LLMè°ƒç”¨ï¼ŒEmbeddingå¯èƒ½éœ€è¦

os.environ["OPENAI_API_KEY"] = API_KEY
EMBEDDING_API_BASE = ""
# --- Prompt æ¨¡æ¿å®šä¹‰ ---
PROMPT_VANILLA = """
You are a biomedical expert. Answer the following question based on your internal knowledge.

Question: {question}
"""

PROMPT_COT = """
You are a biomedical expert. Answer the following question. First, provide your step-by-step reasoning process. Then, provide the final answer.

Let's think step by step.

Question: {question}

Reasoning:
[Your step-by-step reasoning here]

Final Answer:
[Your answer here]
"""

PROMPT_ROT = """
You are a biomedical expert. Answer the following question. 

Question: {question}

Imagine 3 medical experts are solving this task. Each expert independently provides their step-by-step reasoning and final answer.
After all experts have finished, they discuss together, review and backtrack their previous reasoning steps, and finally reach a consensus on the final answer.
Please present:
[Expert 1's reasoning and answer],
[Expert 2's reasoning and answer],
[Expert 3's reasoning and answer],
[The discussion and the agreed final answer]
"""

RAG_TEMPLATE = """
You are a biomedical expert. Answer the following question based ONLY on the provided clinical guideline context.

Question: {question}

Context:
---
{context}
---
"""

# =================================================================
# 3. å‘é‡æ•°æ®åº“ (Vector Database) åˆ›å»ºä¸åŠ è½½
# =================================================================
def create_and_save_vector_db():
    print("--- æ­¥éª¤ 1/3: æœªå‘ç°æœ¬åœ°å‘é‡æ•°æ®åº“ï¼Œå¼€å§‹åˆ›å»º... ---")
    pdf_paths = glob.glob('Knowledge_Sources/*.pdf')
    print(f"  > å‘ç° {len(pdf_paths)} ä¸ªPDFæ–‡ä»¶: {[os.path.basename(p) for p in pdf_paths]}")
    
    all_documents = []
    for pdf_path in pdf_paths:
        loader = PyPDFLoader(pdf_path)
        documents = loader.load()
        all_documents.extend(documents)
    print(f'  > PDFåŠ è½½å®Œæˆï¼Œå…±åˆ‡åˆ†æˆ {len(all_documents)} ä¸ªæ–‡æ¡£é¡µé¢ã€‚')

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    texts = text_splitter.split_documents(all_documents)
    print(f'  > æ–‡æ¡£å—åˆ‡åˆ†å®Œæˆï¼Œå…±å¾—åˆ° {len(texts)} ä¸ªæ–‡æœ¬å—ã€‚')

    print("  > å¼€å§‹åˆ›å»ºå¹¶ä¿å­˜å‘é‡æ•°æ®åº“ï¼Œè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´...")
    vectordb = Chroma.from_documents(
        documents=texts,
        embedding=OpenAIEmbeddings(model=EMBEDDING_MODEL, openai_api_base=EMBEDDING_API_BASE), # æŒ‡å®šæ¨¡å‹
        persist_directory=PERSIST_DIRECTORY
    )
    vectordb.persist()
    print("--- å‘é‡æ•°æ®åº“åˆ›å»ºå¹¶ä¿å­˜æˆåŠŸï¼ ---")
    return vectordb

def load_vector_db():
    print("--- æ­¥éª¤ 1/3: å‘ç°å·²ä¿å­˜çš„å‘é‡æ•°æ®åº“ï¼Œç›´æ¥åŠ è½½... ---")
    vectordb = Chroma(
        persist_directory=PERSIST_DIRECTORY,
        embedding_function=OpenAIEmbeddings(model=EMBEDDING_MODEL, openai_api_base=EMBEDDING_API_BASE) # æŒ‡å®šæ¨¡å‹
    )
    metadatas = vectordb.get().get('metadatas', [])
    if metadatas:
        sources = set(m.get('source') for m in metadatas if 'source' in m)
        print("  > ç”¨äºæ£€ç´¢çš„æ–‡ä»¶:", [os.path.basename(s) for s in sources])
    else:
        print("  > å‘é‡æ•°æ®åº“ä¸ºç©ºæˆ–æ— æ³•è·å–å…ƒæ•°æ®ã€‚")
    print("--- å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸï¼ ---")
    return vectordb

if not os.path.exists(PERSIST_DIRECTORY):
    vectordb = create_and_save_vector_db()
else:
    vectordb = load_vector_db()

# =================================================================
# 4. API è°ƒç”¨å‡½æ•° (ç»Ÿä¸€ä½¿ç”¨ requests)
# =================================================================

# ã€å·²ä¿®æ”¹ã€‘è¿™æ˜¯ä½ éªŒè¯è¿‡æˆåŠŸçš„APIè°ƒç”¨å‡½æ•°ï¼Œæˆ‘ä»¬ç”¨å®ƒæ¥å¤„ç†æ‰€æœ‰éRAGçš„è¯·æ±‚
def fetch_prompts_response(question, PROMPT, max_retries=MAX_RETRIES, sleep_duration=2):
    content = PROMPT.format(question=question)
    headers = {
       'Authorization': f'Bearer {API_KEY}',
       'Content-Type': 'application/json'
    }
    payload = json.dumps({
        "model": LLM_MODEL,
        "messages": [{"role": "user", "content": content}],
        "max_tokens": 4096,
        "temperature": TEMPERATURE,
        "stream": False
    })

    for attempt in range(max_retries):
        try:
            response = requests.post(API_URL, headers=headers, data=payload, timeout=180)
            response.raise_for_status()
            response_data = response.json()
            if 'choices' in response_data and len(response_data['choices']) > 0:
                return response_data['choices'][0]['message']['content'].strip()
            else:
                raise ValueError("APIè¿”å›æ ¼å¼ä¸æ­£ç¡®")
        except Exception as e:
            logging.error(f"API è°ƒç”¨å¤±è´¥ (Attempt {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(sleep_duration)
            else:
                return f"ERROR: API call failed after {max_retries} retries." # è¿”å›é”™è¯¯ä¿¡æ¯è€Œä¸æ˜¯å´©æºƒ
    return None

# ã€å·²ä¿®æ”¹ã€‘è¿™æ˜¯æ–°çš„RAGå“åº”å‡½æ•°ï¼Œå®ƒæ‰‹åŠ¨æ‰§è¡Œæ£€ç´¢ã€æ„å»ºPromptã€ç„¶åè°ƒç”¨ä¸Šé¢çš„å‡½æ•°
def fetch_RAG_response(question, max_retries=MAX_RETRIES):
    try:
        # æ­¥éª¤ 1: ä½¿ç”¨Langchainçš„retrieverè¿›è¡Œæ–‡æ¡£æ£€ç´¢
        retriever = vectordb.as_retriever(search_type="similarity", search_kwargs={"k": 3})
        retrieved_docs = retriever.get_relevant_documents(question)
        
        # æ­¥éª¤ 2: å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹æ ¼å¼åŒ–ä¸ºä¸Šä¸‹æ–‡
        context = "\n\n---\n\n".join([doc.page_content for doc in retrieved_docs])
        
        # æ­¥éª¤ 3: ä½¿ç”¨æˆ‘ä»¬è‡ªå·±çš„APIå‡½æ•°æ¥è·å–å›ç­”
        # æ³¨æ„è¿™é‡Œæˆ‘ä»¬ä¼ å…¥çš„æ˜¯RAG_TEMPLATEï¼Œè€Œä¸æ˜¯å…¶ä»–æ¨¡æ¿
        answer = fetch_prompts_response(
            question=question,
            PROMPT=RAG_TEMPLATE.format(question="{question}", context=context), # æ‰‹åŠ¨å¡«å……context
            max_retries=max_retries
        )
        return answer
    except Exception as e:
        logging.error(f"RAGæµç¨‹å¤±è´¥: {e}")
        return f"ERROR: RAG process failed: {e}"

# =================================================================
# 5. ä¸»æ‰§è¡Œé€»è¾‘ (Main Execution Logic)
# =================================================================
print(f"\n--- æ­¥éª¤ 2/3: å¼€å§‹ä½¿ç”¨æ¨¡å‹ '{LLM_MODEL}' å¤„ç†Benchmark... ---")
basic_benchmark = pd.read_excel(BENCHMARK_FILE, sheet_name="Sheet1")
results = []

for index, row in tqdm(basic_benchmark.iterrows(), total=basic_benchmark.shape[0], desc="å¤„ç†Benchmarké—®é¢˜"):
    question_type = row['type']
    question = str(row['question']) # ç¡®ä¿é—®é¢˜æ˜¯å­—ç¬¦ä¸²
    full_question = f"{question_type}: {question}" # ç»„åˆé—®é¢˜ç±»å‹å’Œå†…å®¹
    domain_topic = row['Domain_Topic']
    ground_truth = row['answer']

    print(f"\nå¤„ç†é—®é¢˜ {index + 1}/{len(basic_benchmark)}: {full_question[:100]}...") # æ‰“å°æˆªæ–­çš„é—®é¢˜ä»¥ä¿æŒæ•´æ´

    print("  > è·å– [Vanilla] å›ç­”...")
    vanilla_response = fetch_prompts_response(full_question, PROMPT_VANILLA)
    
    print("  > è·å– [CoT] å›ç­”...")
    cot_response = fetch_prompts_response(full_question, PROMPT_COT)
    
    print("  > è·å– [RoT] å›ç­”...")
    rot_response = fetch_prompts_response(full_question, PROMPT_ROT)
    
    print("  > è·å– [RAG] å›ç­”...")
    rag_response = fetch_RAG_response(full_question)

    results.append({
        "type": question_type,
        "question": question,
        "domain_topic": domain_topic,
        "ground_truth": ground_truth,
        "Vanilla_response": vanilla_response,
        "COT_response": cot_response,
        "ROT_response": rot_response,
        "RAG_response": rag_response,
    })
    
    # ã€é‡è¦æç¤ºã€‘: ä¸‹é¢çš„ 'break' æ˜¯ä¸ºäº†å¿«é€Ÿæµ‹è¯•ã€‚
    # å½“ä½ å‡†å¤‡å¥½è¦è¿è¡Œæ‰€æœ‰é—®é¢˜æ—¶ï¼Œè¯·åŠ¡å¿…åˆ é™¤æˆ–æ³¨é‡Šæ‰å®ƒï¼
    #break

# =================================================================
# 6. ä¿å­˜ç»“æœ
# =================================================================
print(f"\n--- æ­¥éª¤ 3/3: æ‰€æœ‰é—®é¢˜å¤„ç†å®Œæ¯•ï¼Œæ­£åœ¨ä¿å­˜ç»“æœ... ---")
df = pd.DataFrame(results)
df.to_csv(OUTPUT_FILENAME, index=False, encoding="utf-8-sig")
print(f"ğŸ‰ ä»»åŠ¡å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³: {OUTPUT_FILENAME}")